{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrmA3eaj3kF0"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"Mail korechi dekhno pdf ta thik ache?\n",
        "Kiser?\n",
        "Fee payment üòÇ\n",
        "Ha thik a6e shudu chul gulo besi ure gia6e\n",
        "ü•≤ü•≤ü•≤\n",
        "Ager photo pali nai\n",
        "<Media omitted>\n",
        "You deleted this message\n",
        "<Media omitted>\n",
        "Vill-KECHARKUR, DIST- SOUTH 24 PGS, PIN-743395\n",
        "https://youtu.be/rKv98tE4De8\n",
        "<Media omitted>\n",
        "https://www.figma.com/community/file/1202113647978083142\n",
        "Bol\n",
        "Biswjit library asbe ki?\n",
        "Na\n",
        "Ok\n",
        "Ha\n",
        "Tui ki NSS er program e jabi?\n",
        "No\n",
        "Call korbi too argent\n",
        "Class e a6i\n",
        "Ok\n",
        "Ses hole koris\n",
        "Ami hostel e achi\n",
        "Quality ta bariye dis\n",
        "ok\n",
        "http://sakil.loogbyte.in/\n",
        "Eta 1080p dwanload kore dis\n",
        "This message was deleted\n",
        "eta 1080 dwanload kore dis to\n",
        "Dept te gia di66i\n",
        "Akhane net slow\n",
        "egulu baad diye r kichu bol\n",
        "Assignment ta hoye gele amake Patas too\n",
        "Amake korte hobe\n",
        "Ok\n",
        "16 september confirm karon tar age korle msc ra asbe na\n",
        "R 16 tarikh ta amader dada rao bolche so etai\n",
        "Ok\n",
        "Date conform kor\n",
        "Pore janabo dishani di k\n",
        "Ok\n",
        "eta dwanload kore de to 720 amar speed dichhe na\n",
        "Cl korbi to\n",
        "4 din age beria6e\n",
        "hm eta dekhlam ami kichuta valo ache\n",
        "ha\n",
        "3 number ta kore dibi too\n",
        "?\n",
        "??\n",
        "3 number ta dwanload kore de\n",
        "apply korbo ??\n",
        "Link ta open hoche koi\n",
        "khul6e to dhak ??\n",
        "but etar website e kichu nei\n",
        "dhakli ??\n",
        "apply korbo ki ??\n",
        "Hmm apply kore de\n",
        "tui ki korbi ??\n",
        "Korbo laptop\n",
        "‚òπÔ∏è‚òπÔ∏è resume nei\n",
        "Kor kotokhon time lagbe\n",
        "Faka time e lupin ta dekhbi mst ache\n",
        "Ok\n",
        "Bari jayar din dhakbo\n",
        "Ok\n",
        "Better luck next time üòî\n",
        "Tui apply korli ?\n",
        "Tui korle peye jabi\n",
        "Na kori nai vule gechli\n",
        "Resume korli\n",
        "Ha ota dheke dei ni\n",
        "Dekha\n",
        "R 200 taka de too recharge korbo r add kore dis taka ta\n",
        "Barite 30000 taka dia6i re amr kache tmn taka nei\n",
        "Bujli\n",
        "Ok ami  onno karo kache nichhi\n",
        "Tor mouse ta without keyboard e neoya jabe?\n",
        "Na\n",
        "Onno kono\n",
        "Ha\n",
        "Jomi er Record online e bar kora jai naki?\n",
        " Hmm record kora thakle copy ber kora jabe\n",
        " Ki kore?\n",
        " Ar atar kono hard copy dei naki?\n",
        " Taka pay korte Hobe tahole pabi\n",
        " Plot or khotiyan number ki ache?\n",
        " Jani na\n",
        " Dolil e dekhbi lekha ache\n",
        " Ok\n",
        " ei suru korechi ami\n",
        " Bhh\n",
        " Loki Somoy Rokshok\n",
        " Bol6i tor room er chabi ki amader room e\n",
        " ?\n",
        " Ami 20 tarikh train e uthte parbo na\n",
        "Amr mejdar dengue hoye6e\n",
        " Bari theke aktu deri kore jete bol6e\n",
        "Hm\n",
        " Tao Kobe parbi\n",
        " Tui ja 20 tarikh dorker hole lock veye dis\n",
        " Ok\n",
        " null\n",
        " Tui ekta link patha too key unlock korar\n",
        " Dhak kichu bujte Paris ki na\n",
        " null\n",
        " Bhh\n",
        " Beta\n",
        " Ektu side e ghurte gechilam üòÇ\n",
        " Bhh\n",
        " 2 jon?\n",
        " üòî R EKJON ACHE BUT ALADA BIKE\n",
        " Ooo\n",
        " Bishal baper too\n",
        " Hm\n",
        " Bhh\n",
        " Call korbo? Basto a6is?\n",
        " null\n",
        " Faltu bothe anty virus amar 3 yr chilo quick heal\n",
        " Oo\n",
        " Ok\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "J1D42emD32Ro"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "KhtDxwL_AXFj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "K8MRFre9AaG9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "TMIQITCT2Xn8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrpAl3EDAgvh",
        "outputId": "ac8db5c3-685a-447c-c720-10cd1e2c2d03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "44VahqKdAjr9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyqwPDzNA5mR",
        "outputId": "e5c48840-9456-4cea-84c0-7869b3257d1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[79, 35],\n",
              " [79, 35, 80],\n",
              " [79, 35, 80, 81],\n",
              " [79, 35, 80, 81, 3],\n",
              " [79, 35, 80, 81, 3, 36],\n",
              " [79, 35, 80, 81, 3, 36, 7],\n",
              " [83, 84],\n",
              " [83, 84, 37],\n",
              " [10, 36],\n",
              " [10, 36, 85],\n",
              " [10, 36, 85, 86],\n",
              " [10, 36, 85, 86, 87],\n",
              " [10, 36, 85, 86, 87, 88],\n",
              " [10, 36, 85, 86, 87, 88, 89],\n",
              " [10, 36, 85, 86, 87, 88, 89, 90],\n",
              " [10, 36, 85, 86, 87, 88, 89, 90, 91],\n",
              " [93, 94],\n",
              " [93, 94, 95],\n",
              " [93, 94, 95, 38],\n",
              " [22, 23],\n",
              " [96, 39],\n",
              " [96, 39, 40],\n",
              " [96, 39, 40, 41],\n",
              " [22, 23],\n",
              " [97, 98],\n",
              " [97, 98, 99],\n",
              " [97, 98, 99, 100],\n",
              " [97, 98, 99, 100, 101],\n",
              " [97, 98, 99, 100, 101, 102],\n",
              " [97, 98, 99, 100, 101, 102, 103],\n",
              " [97, 98, 99, 100, 101, 102, 103, 104],\n",
              " [42, 105],\n",
              " [42, 105, 106],\n",
              " [42, 105, 106, 107],\n",
              " [22, 23],\n",
              " [42, 108],\n",
              " [42, 108, 109],\n",
              " [42, 108, 109, 110],\n",
              " [42, 108, 109, 110, 111],\n",
              " [42, 108, 109, 110, 111, 112],\n",
              " [42, 108, 109, 110, 111, 112, 113],\n",
              " [114, 115],\n",
              " [114, 115, 44],\n",
              " [114, 115, 44, 5],\n",
              " [8, 5],\n",
              " [8, 5, 116],\n",
              " [8, 5, 116, 24],\n",
              " [8, 5, 116, 24, 117],\n",
              " [8, 5, 116, 24, 117, 2],\n",
              " [8, 5, 116, 24, 117, 2, 45],\n",
              " [46, 25],\n",
              " [46, 25, 9],\n",
              " [46, 25, 9, 119],\n",
              " [120, 2],\n",
              " [120, 2, 121],\n",
              " [122, 47],\n",
              " [122, 47, 123],\n",
              " [11, 124],\n",
              " [11, 124, 2],\n",
              " [11, 124, 2, 125],\n",
              " [126, 3],\n",
              " [126, 3, 127],\n",
              " [126, 3, 127, 12],\n",
              " [128, 129],\n",
              " [128, 129, 130],\n",
              " [128, 129, 130, 131],\n",
              " [16, 132],\n",
              " [16, 132, 17],\n",
              " [16, 132, 17, 4],\n",
              " [16, 132, 17, 4, 12],\n",
              " [40, 41],\n",
              " [40, 41, 133],\n",
              " [40, 41, 133, 39],\n",
              " [16, 134],\n",
              " [16, 134, 17],\n",
              " [16, 134, 17, 4],\n",
              " [16, 134, 17, 4, 12],\n",
              " [16, 134, 17, 4, 12, 18],\n",
              " [135, 136],\n",
              " [135, 136, 137],\n",
              " [135, 136, 137, 138],\n",
              " [139, 140],\n",
              " [139, 140, 141],\n",
              " [142, 143],\n",
              " [142, 143, 144],\n",
              " [142, 143, 144, 13],\n",
              " [142, 143, 144, 13, 26],\n",
              " [142, 143, 144, 13, 26, 43],\n",
              " [145, 3],\n",
              " [145, 3, 146],\n",
              " [145, 3, 146, 147],\n",
              " [145, 3, 146, 147, 48],\n",
              " [145, 3, 146, 147, 48, 148],\n",
              " [145, 3, 146, 147, 48, 148, 9],\n",
              " [48, 49],\n",
              " [48, 49, 50],\n",
              " [51, 149],\n",
              " [51, 149, 150],\n",
              " [51, 149, 150, 151],\n",
              " [51, 149, 150, 151, 152],\n",
              " [51, 149, 150, 151, 152, 52],\n",
              " [51, 149, 150, 151, 152, 52, 53],\n",
              " [51, 149, 150, 151, 152, 52, 53, 153],\n",
              " [51, 149, 150, 151, 152, 52, 53, 153, 154],\n",
              " [51, 149, 150, 151, 152, 52, 53, 153, 154, 44],\n",
              " [51, 149, 150, 151, 152, 52, 53, 153, 154, 44, 6],\n",
              " [13, 51],\n",
              " [13, 51, 27],\n",
              " [13, 51, 27, 3],\n",
              " [13, 51, 27, 3, 54],\n",
              " [13, 51, 27, 3, 54, 155],\n",
              " [13, 51, 27, 3, 54, 155, 156],\n",
              " [13, 51, 27, 3, 54, 155, 156, 157],\n",
              " [13, 51, 27, 3, 54, 155, 156, 157, 158],\n",
              " [13, 51, 27, 3, 54, 155, 156, 157, 158, 159],\n",
              " [160, 161],\n",
              " [160, 161, 55],\n",
              " [162, 163],\n",
              " [162, 163, 164],\n",
              " [162, 163, 164, 165],\n",
              " [162, 163, 164, 165, 166],\n",
              " [16, 17],\n",
              " [16, 17, 4],\n",
              " [16, 17, 4, 19],\n",
              " [16, 17, 4, 19, 18],\n",
              " [16, 17, 4, 19, 18, 167],\n",
              " [16, 17, 4, 19, 18, 167, 56],\n",
              " [16, 17, 4, 19, 18, 167, 56, 168],\n",
              " [16, 17, 4, 19, 18, 167, 56, 168, 169],\n",
              " [16, 17, 4, 19, 18, 167, 56, 168, 169, 6],\n",
              " [170, 25],\n",
              " [170, 25, 18],\n",
              " [171, 57],\n",
              " [171, 57, 52],\n",
              " [171, 57, 52, 172],\n",
              " [28, 16],\n",
              " [28, 16, 173],\n",
              " [28, 16, 173, 11],\n",
              " [28, 16, 173, 11, 174],\n",
              " [28, 16, 173, 11, 174, 175],\n",
              " [28, 16, 173, 11, 174, 175, 7],\n",
              " [29, 30],\n",
              " [29, 30, 3],\n",
              " [29, 30, 3, 4],\n",
              " [29, 30, 3, 4, 176],\n",
              " [29, 30, 3, 4, 176, 9],\n",
              " [29, 30],\n",
              " [29, 30, 3],\n",
              " [29, 30, 3, 17],\n",
              " [29, 30, 3, 17, 4],\n",
              " [29, 30, 3, 17, 4, 19],\n",
              " [20, 14],\n",
              " [58, 3],\n",
              " [58, 3, 177],\n",
              " [58, 3, 177, 178],\n",
              " [58, 3, 177, 178, 179],\n",
              " [180, 18],\n",
              " [180, 18, 59],\n",
              " [60, 181],\n",
              " [60, 181, 182],\n",
              " [60, 181, 182, 2],\n",
              " [60, 181, 182, 2, 26],\n",
              " [60, 181, 182, 2, 26, 31],\n",
              " [20, 14],\n",
              " [20, 14, 5],\n",
              " [61, 20],\n",
              " [61, 20, 4],\n",
              " [61, 20, 4, 19],\n",
              " [8, 5],\n",
              " [8, 5, 25],\n",
              " [14, 184],\n",
              " [185, 62],\n",
              " [185, 62, 31],\n",
              " [55, 186],\n",
              " [55, 186, 32],\n",
              " [55, 186, 32, 187],\n",
              " [188, 32],\n",
              " [188, 32, 2],\n",
              " [188, 32, 2, 189],\n",
              " [188, 32, 2, 189, 3],\n",
              " [188, 32, 2, 189, 3, 63],\n",
              " [188, 32, 2, 189, 3, 63, 190],\n",
              " [188, 32, 2, 189, 3, 63, 190, 7],\n",
              " [64, 191],\n",
              " [64, 191, 57],\n",
              " [64, 191, 57, 192],\n",
              " [193, 194],\n",
              " [193, 194, 195],\n",
              " [193, 194, 195, 32],\n",
              " [193, 194, 195, 32, 65],\n",
              " [8, 20],\n",
              " [8, 20, 66],\n",
              " [8, 53],\n",
              " [8, 53, 196],\n",
              " [8, 53, 196, 45],\n",
              " [6, 197],\n",
              " [6, 197, 38],\n",
              " [6, 197, 38, 198],\n",
              " [6, 197, 38, 198, 199],\n",
              " [62, 66],\n",
              " [10, 200],\n",
              " [10, 200, 201],\n",
              " [10, 200, 201, 67],\n",
              " [10, 200, 201, 67, 202],\n",
              " [13, 204],\n",
              " [13, 204, 15],\n",
              " [13, 204, 15, 19],\n",
              " [13, 204, 15, 19, 9],\n",
              " [13, 204, 15, 19, 9, 205],\n",
              " [13, 204, 15, 19, 9, 205, 14],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13, 206],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13, 206, 4],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13, 206, 4, 12],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13, 206, 4, 12, 15],\n",
              " [13, 204, 15, 19, 9, 205, 14, 13, 206, 4, 12, 15, 3],\n",
              " [207, 208],\n",
              " [207, 208, 15],\n",
              " [207, 208, 15, 209],\n",
              " [207, 208, 15, 209, 210],\n",
              " [207, 208, 15, 209, 210, 68],\n",
              " [207, 208, 15, 209, 210, 68, 69],\n",
              " [207, 208, 15, 209, 210, 68, 69, 211],\n",
              " [207, 208, 15, 209, 210, 68, 69, 211, 15],\n",
              " [207, 208, 15, 209, 210, 68, 69, 211, 15, 31],\n",
              " [1, 11],\n",
              " [1, 11, 70],\n",
              " [1, 11, 70, 213],\n",
              " [1, 11, 70, 213, 69],\n",
              " [1, 11, 70, 213, 69, 214],\n",
              " [71, 215],\n",
              " [71, 215, 3],\n",
              " [71, 215, 3, 216],\n",
              " [71, 215, 3, 216, 217],\n",
              " [71, 215, 3, 216, 217, 2],\n",
              " [71, 215, 3, 216, 217, 2, 218],\n",
              " [71, 215, 3, 216, 217, 2, 218, 72],\n",
              " [70, 73],\n",
              " [219, 24],\n",
              " [219, 24, 74],\n",
              " [219, 24, 74, 220],\n",
              " [219, 24, 74, 220, 2],\n",
              " [219, 24, 74, 220, 2, 221],\n",
              " [219, 24, 74, 220, 2, 221, 33],\n",
              " [219, 24, 74, 220, 2, 221, 33, 222],\n",
              " [219, 24, 74, 220, 2, 221, 33, 222, 75],\n",
              " [61, 74],\n",
              " [61, 74, 33],\n",
              " [61, 74, 33, 223],\n",
              " [61, 74, 33, 223, 76],\n",
              " [61, 74, 33, 223, 76, 224],\n",
              " [61, 74, 33, 223, 76, 224, 33],\n",
              " [61, 74, 33, 223, 76, 224, 33, 72],\n",
              " [5, 4],\n",
              " [225, 226],\n",
              " [225, 226, 73],\n",
              " [225, 226, 73, 227],\n",
              " [225, 226, 73, 227, 76],\n",
              " [225, 226, 73, 227, 76, 67],\n",
              " [225, 226, 73, 227, 76, 67, 75],\n",
              " [15, 228],\n",
              " [15, 228, 49],\n",
              " [15, 228, 49, 50],\n",
              " [15, 228, 49, 50, 229],\n",
              " [15, 228, 49, 50, 229, 230],\n",
              " [231, 232],\n",
              " [231, 232, 233],\n",
              " [231, 232, 233, 30],\n",
              " [231, 232, 233, 30, 5],\n",
              " [231, 232, 233, 30, 5, 7],\n",
              " [234, 6],\n",
              " [235, 2],\n",
              " [235, 2, 63],\n",
              " [235, 2, 63, 236],\n",
              " [235, 2, 63, 236, 7],\n",
              " [237, 238],\n",
              " [237, 238, 35],\n",
              " [237, 238, 35, 11],\n",
              " [239, 240],\n",
              " [239, 240, 241],\n",
              " [242, 71],\n",
              " [242, 71, 77],\n",
              " [242, 71, 77, 24],\n",
              " [242, 71, 77, 24, 243],\n",
              " [242, 71, 77, 24, 243, 5],\n",
              " [242, 71, 77, 24, 243, 5, 54],\n",
              " [242, 71, 77, 24, 243, 5, 54, 77],\n",
              " [242, 71, 77, 24, 243, 5, 54, 77, 2],\n",
              " [11, 78],\n",
              " [11, 78, 27],\n",
              " [11, 78, 27, 244],\n",
              " [11, 78, 27, 244, 2],\n",
              " [11, 78, 27, 244, 2, 245],\n",
              " [11, 78, 27, 244, 2, 245, 246],\n",
              " [11, 78, 27, 244, 2, 245, 246, 6],\n",
              " [68, 247],\n",
              " [68, 247, 248],\n",
              " [68, 247, 248, 249],\n",
              " [64, 250],\n",
              " [64, 250, 251],\n",
              " [64, 250, 251, 252],\n",
              " [64, 250, 251, 252, 4],\n",
              " [64, 250, 251, 252, 4, 253],\n",
              " [64, 250, 251, 252, 4, 253, 254],\n",
              " [255, 256],\n",
              " [255, 256, 257],\n",
              " [8, 258],\n",
              " [8, 258, 78],\n",
              " [8, 258, 78, 27],\n",
              " [8, 258, 78, 27, 259],\n",
              " [8, 258, 78, 27, 259, 47],\n",
              " [8, 258, 78, 27, 259, 47, 260],\n",
              " [8, 258, 78, 27, 259, 47, 260, 261],\n",
              " [8, 258, 78, 27, 259, 47, 260, 261, 12],\n",
              " [8, 262],\n",
              " [8, 262, 58],\n",
              " [8, 262, 58, 263],\n",
              " [8, 262, 58, 263, 9],\n",
              " [8, 262, 58, 263, 9, 264],\n",
              " [8, 262, 58, 263, 9, 264, 265],\n",
              " [8, 262, 58, 263, 9, 264, 265, 266],\n",
              " [59, 26],\n",
              " [59, 26, 267],\n",
              " [59, 26, 267, 268],\n",
              " [59, 26, 267, 268, 5],\n",
              " [59, 26, 267, 268, 5, 6],\n",
              " [270, 271],\n",
              " [270, 271, 2],\n",
              " [270, 271, 2, 272],\n",
              " [270, 271, 2, 272, 273],\n",
              " [270, 271, 2, 272, 273, 37],\n",
              " [274, 275],\n",
              " [65, 13],\n",
              " [65, 13, 276],\n",
              " [65, 13, 276, 7],\n",
              " [65, 13, 276, 7, 60],\n",
              " [65, 13, 276, 7, 60, 277],\n",
              " [65, 13, 276, 7, 60, 277, 278],\n",
              " [280, 281],\n",
              " [280, 281, 9],\n",
              " [46, 14],\n",
              " [46, 14, 282],\n",
              " [46, 14, 282, 283],\n",
              " [284, 285],\n",
              " [284, 285, 286],\n",
              " [284, 285, 286, 287],\n",
              " [284, 285, 286, 287, 56],\n",
              " [284, 285, 286, 287, 56, 29],\n",
              " [284, 285, 286, 287, 56, 29, 288],\n",
              " [284, 285, 286, 287, 56, 29, 288, 289],\n",
              " [284, 285, 286, 287, 56, 29, 288, 289, 290],\n",
              " [284, 285, 286, 287, 56, 29, 288, 289, 290, 291]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "CrzbvUUQCXPU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "9oPMoWBSD1_U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miRb-QZyIi7_",
        "outputId": "e8c81c81-9d96-443a-bf22-e6ab3f9737bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  79,  35],\n",
              "       [  0,   0,   0, ...,  79,  35,  80],\n",
              "       [  0,   0,   0, ...,  35,  80,  81],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  29, 288, 289],\n",
              "       [  0,   0,   0, ..., 288, 289, 290],\n",
              "       [  0,   0,   0, ..., 289, 290, 291]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "qVI0-UUrIsd3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "lXrYHTDFI3uE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmsFnHx1Qdow",
        "outputId": "747d3694-332f-4fa1-9c38-8c090246610d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wyYqYgZSeck",
        "outputId": "2eaee940-b936-43bd-9bca-0ff42ad4f59a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=num_words)"
      ],
      "metadata": {
        "id": "rs1NPitwSgzk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQMJ0I6xSiZf",
        "outputId": "1e964ace-fe5d-4011-b085-ccc33e791cf7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352, 293)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "9kVeTvR2S8Fk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "lstm_units = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, input_length=max_len - 1))\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(num_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "wo-OYfHpTK2o"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-GGjqh7ue_Yq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxxXkrSXfIBv",
        "outputId": "31f445f6-cbfe-447d-93e6-fb5aed6bcc48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 12, 100)           29300     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 12, 150)           150600    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 293)               44243     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404743 (1.54 MB)\n",
            "Trainable params: 404743 (1.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "mbWfuhJ687P-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpFUCALCfJRR",
        "outputId": "16e916af-2590-410f-905c-09bfb7771f64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 5s 57ms/step - loss: 5.6770 - accuracy: 0.0256\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 5.5704 - accuracy: 0.0312\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 5.4155 - accuracy: 0.0199\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 5.3289 - accuracy: 0.0312\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 5.2951 - accuracy: 0.0227\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 5.2753 - accuracy: 0.0284\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 93ms/step - loss: 5.2644 - accuracy: 0.0312\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 5.2464 - accuracy: 0.0341\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 5.2236 - accuracy: 0.0341\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 5.1931 - accuracy: 0.0369\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 5.1358 - accuracy: 0.0398\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 5.0607 - accuracy: 0.0341\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 4.9598 - accuracy: 0.0398\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 4.8549 - accuracy: 0.0511\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 4.7442 - accuracy: 0.0568\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 4.6380 - accuracy: 0.0483\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 4.5114 - accuracy: 0.0511\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 4.4142 - accuracy: 0.0483\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 4.3092 - accuracy: 0.0739\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 4.1909 - accuracy: 0.0938\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 4.0891 - accuracy: 0.0881\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 4.0033 - accuracy: 0.1023\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 3.8981 - accuracy: 0.1278\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 3.7975 - accuracy: 0.1193\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 3.7124 - accuracy: 0.1420\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 3.6238 - accuracy: 0.1733\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 3.5299 - accuracy: 0.1818\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 3.4496 - accuracy: 0.2330\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 3.3777 - accuracy: 0.2188\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 1s 81ms/step - loss: 3.3107 - accuracy: 0.2415\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 3.2376 - accuracy: 0.2642\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 3.1704 - accuracy: 0.2642\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 3.1177 - accuracy: 0.2727\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 3.0600 - accuracy: 0.3210\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.9968 - accuracy: 0.3324\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.9206 - accuracy: 0.3494\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 2.8437 - accuracy: 0.3665\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 2.7676 - accuracy: 0.3949\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.7095 - accuracy: 0.3949\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.6343 - accuracy: 0.4176\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 2.5759 - accuracy: 0.4460\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 2.5321 - accuracy: 0.4261\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.4880 - accuracy: 0.4403\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 2.4295 - accuracy: 0.4744\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.3663 - accuracy: 0.4801\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 1s 66ms/step - loss: 2.3113 - accuracy: 0.4943\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 2.2719 - accuracy: 0.5114\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 2.2345 - accuracy: 0.5170\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 2.1835 - accuracy: 0.5142\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 2.1311 - accuracy: 0.5511\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 2.0816 - accuracy: 0.5682\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.0283 - accuracy: 0.5511\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.0005 - accuracy: 0.5682\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.9454 - accuracy: 0.6023\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.9069 - accuracy: 0.6080\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.8708 - accuracy: 0.6307\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.8163 - accuracy: 0.6392\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.7840 - accuracy: 0.6364\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.7450 - accuracy: 0.6449\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 1.7108 - accuracy: 0.6477\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.6679 - accuracy: 0.6676\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.6378 - accuracy: 0.6676\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.6083 - accuracy: 0.6818\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.5980 - accuracy: 0.6875\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.5643 - accuracy: 0.6847\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.5247 - accuracy: 0.7074\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 1.4851 - accuracy: 0.7216\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 1.4418 - accuracy: 0.7301\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 1s 107ms/step - loss: 1.4064 - accuracy: 0.7386\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 1.3710 - accuracy: 0.7301\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 1s 74ms/step - loss: 1.3445 - accuracy: 0.7330\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.3145 - accuracy: 0.7670\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.2938 - accuracy: 0.7557\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.2587 - accuracy: 0.7727\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.2315 - accuracy: 0.7869\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.2081 - accuracy: 0.7898\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.1821 - accuracy: 0.7955\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.1517 - accuracy: 0.8153\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.1240 - accuracy: 0.8040\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.0989 - accuracy: 0.8011\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.0798 - accuracy: 0.8125\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.0576 - accuracy: 0.8068\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.0357 - accuracy: 0.8125\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.0251 - accuracy: 0.8239\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.0041 - accuracy: 0.8295\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.9790 - accuracy: 0.8267\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.9653 - accuracy: 0.8438\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.9363 - accuracy: 0.8466\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.9114 - accuracy: 0.8494\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 1s 92ms/step - loss: 0.8920 - accuracy: 0.8438\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.8689 - accuracy: 0.8580\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.8515 - accuracy: 0.8494\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.8322 - accuracy: 0.8608\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.8206 - accuracy: 0.8636\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.7991 - accuracy: 0.8580\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.7807 - accuracy: 0.8750\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.7695 - accuracy: 0.8722\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.7529 - accuracy: 0.8949\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.7359 - accuracy: 0.8778\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.7228 - accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a5d94573c10>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \"Tui\"\n",
        "\n",
        "for i in range(3):\n",
        "    token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "    padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding='pre')\n",
        "    pos = np.argmax(model.predict(padded_token_text))\n",
        "    predicted_word = tokenizer.index_word[pos]\n",
        "    text = text + \" \" + predicted_word\n",
        "    print(text)\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGeYGwCMfTus",
        "outputId": "fd3cf8b1-c684-4b2d-8b51-4f81bc42634e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Tui ki\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Tui ki korbi\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Tui ki korbi too\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxsj-_CjbQW",
        "outputId": "7e8e0b43-d8f5-4e87-8caf-33806965ab3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'you': 2,\n",
              " 'i': 3,\n",
              " 'to': 4,\n",
              " 'a': 5,\n",
              " 'of': 6,\n",
              " 'is': 7,\n",
              " 'have': 8,\n",
              " 'will': 9,\n",
              " 'can': 10,\n",
              " 'what': 11,\n",
              " 'course': 12,\n",
              " 'program': 13,\n",
              " 'in': 14,\n",
              " 'for': 15,\n",
              " 'all': 16,\n",
              " 'sessions': 17,\n",
              " 'on': 18,\n",
              " 'be': 19,\n",
              " 'and': 20,\n",
              " 'this': 21,\n",
              " 'if': 22,\n",
              " 'am': 23,\n",
              " 'pay': 24,\n",
              " 'payment': 25,\n",
              " 'make': 26,\n",
              " 'we': 27,\n",
              " 'do': 28,\n",
              " 'subscription': 29,\n",
              " 'where': 30,\n",
              " 'rs': 31,\n",
              " 'so': 32,\n",
              " 'campusx': 33,\n",
              " 'session': 34,\n",
              " 'our': 35,\n",
              " 'paid': 36,\n",
              " 'join': 37,\n",
              " 'able': 38,\n",
              " 'your': 39,\n",
              " 'website': 40,\n",
              " 'placement': 41,\n",
              " 'fee': 42,\n",
              " 'data': 43,\n",
              " 'monthly': 44,\n",
              " 'month': 45,\n",
              " 'not': 46,\n",
              " 'get': 47,\n",
              " 'yes': 48,\n",
              " 'once': 49,\n",
              " 'past': 50,\n",
              " 'feb': 51,\n",
              " 'assistance': 52,\n",
              " 'science': 53,\n",
              " '7': 54,\n",
              " '5600': 55,\n",
              " 'are': 56,\n",
              " 'watch': 57,\n",
              " 'google': 58,\n",
              " 'by': 59,\n",
              " 'com': 60,\n",
              " 'mail': 61,\n",
              " 'from': 62,\n",
              " 'contact': 63,\n",
              " 'us': 64,\n",
              " 'at': 65,\n",
              " 'or': 66,\n",
              " 'doubt': 67,\n",
              " 'mentorship': 68,\n",
              " 'payments': 69,\n",
              " '799': 70,\n",
              " 'total': 71,\n",
              " 'duration': 72,\n",
              " 'months': 73,\n",
              " 'learning': 74,\n",
              " 'case': 75,\n",
              " 'here': 76,\n",
              " 'https': 77,\n",
              " 'part': 78,\n",
              " 'see': 79,\n",
              " 'late': 80,\n",
              " 'dashboard': 81,\n",
              " 'task': 82,\n",
              " 'don‚Äôt': 83,\n",
              " 'nitish': 84,\n",
              " 'validity': 85,\n",
              " '15th': 86,\n",
              " 'jan': 87,\n",
              " 'period': 88,\n",
              " 'after': 89,\n",
              " 'till': 90,\n",
              " '21st': 91,\n",
              " 'that': 92,\n",
              " 'about': 93,\n",
              " 'follows': 94,\n",
              " 'model': 95,\n",
              " 'syllabus': 96,\n",
              " 'python': 97,\n",
              " 'ml': 98,\n",
              " 'studies': 99,\n",
              " 'learnwith': 100,\n",
              " 'deep': 101,\n",
              " 'nlp': 102,\n",
              " 'no': 103,\n",
              " 'miss': 104,\n",
              " 'live': 105,\n",
              " 'recording': 106,\n",
              " 'even': 107,\n",
              " 'class': 108,\n",
              " 'time': 109,\n",
              " '2': 110,\n",
              " 'how': 111,\n",
              " 'absolutely': 112,\n",
              " 'middle': 113,\n",
              " 'anytime': 114,\n",
              " 'submit': 115,\n",
              " 'with': 116,\n",
              " 'gmail': 117,\n",
              " 'registration': 118,\n",
              " 'related': 119,\n",
              " 'link': 120,\n",
              " 'entire': 121,\n",
              " 'suppose': 122,\n",
              " 'again': 123,\n",
              " 'days': 124,\n",
              " 'day': 125,\n",
              " 'refund': 126,\n",
              " 'living': 127,\n",
              " 'outside': 128,\n",
              " 'india': 129,\n",
              " 'should': 130,\n",
              " 'sending': 131,\n",
              " 'queries': 132,\n",
              " 'videos': 133,\n",
              " 'read': 134,\n",
              " 'but': 135,\n",
              " 'provided': 136,\n",
              " 'form': 137,\n",
              " '1': 138,\n",
              " 'clearance': 139,\n",
              " 'week': 140,\n",
              " 'just': 141,\n",
              " 'certificate': 142,\n",
              " 'earlier': 143,\n",
              " 'comes': 144,\n",
              " 'under': 145,\n",
              " 'guarantee': 146,\n",
              " 'dsmp': 147,\n",
              " '2023': 148,\n",
              " 'becomes': 149,\n",
              " 'approx': 150,\n",
              " 'covering': 151,\n",
              " 'following': 152,\n",
              " 'modules': 153,\n",
              " 'fundamentals': 154,\n",
              " 'libraries': 155,\n",
              " 'analysis': 156,\n",
              " 'sql': 157,\n",
              " 'maths': 158,\n",
              " 'machine': 159,\n",
              " 'algorithms': 160,\n",
              " 'practical': 161,\n",
              " 'mlops': 162,\n",
              " 'check': 163,\n",
              " 'detailed': 164,\n",
              " 'courses': 165,\n",
              " '637339afe4b0615a1bbed390': 166,\n",
              " 'both': 167,\n",
              " 'program‚Äôs': 168,\n",
              " 'curriculum': 169,\n",
              " 'recorded': 170,\n",
              " 'go': 171,\n",
              " 'back': 172,\n",
              " 'find': 173,\n",
              " 'schedule': 174,\n",
              " 'checkout': 175,\n",
              " 'sheet': 176,\n",
              " 'table': 177,\n",
              " 'docs': 178,\n",
              " 'spreadsheets': 179,\n",
              " 'd': 180,\n",
              " '16ootax': 181,\n",
              " 'a6oraecg4emgexhqqpv3noqpyku7rj6arozk': 182,\n",
              " 'edit': 183,\n",
              " 'usp': 184,\n",
              " 'sharing': 185,\n",
              " 'roughly': 186,\n",
              " 'last': 187,\n",
              " 'hours': 188,\n",
              " 'language': 189,\n",
              " 'spoken': 190,\n",
              " 'instructor': 191,\n",
              " 'during': 192,\n",
              " 'hinglish': 193,\n",
              " 'informed': 194,\n",
              " 'upcoming': 195,\n",
              " 'side': 196,\n",
              " 'before': 197,\n",
              " 'every': 198,\n",
              " 'become': 199,\n",
              " 'user': 200,\n",
              " 'non': 201,\n",
              " 'tech': 202,\n",
              " 'background': 203,\n",
              " 'lectures': 204,\n",
              " 'content': 205,\n",
              " 'provide': 206,\n",
              " 'solutions': 207,\n",
              " 'self': 208,\n",
              " 'evaluate': 209,\n",
              " 'yourself': 210,\n",
              " 'questions': 211,\n",
              " 'youtube': 212,\n",
              " 'channel': 213,\n",
              " 'amount': 214,\n",
              " 'unfortunately': 215,\n",
              " 'then': 216,\n",
              " '1st': 217,\n",
              " '30': 218,\n",
              " 'essentially': 219,\n",
              " 'wait': 220,\n",
              " 'end': 221,\n",
              " 'like': 222,\n",
              " 'making': 223,\n",
              " 'policy': 224,\n",
              " 'made': 225,\n",
              " 'post': 226,\n",
              " 'when': 227,\n",
              " 'view': 228,\n",
              " 'one': 229,\n",
              " 'tricky': 230,\n",
              " 'carefully': 231,\n",
              " 'valid': 232,\n",
              " 'purchased': 233,\n",
              " '20th': 234,\n",
              " 'purchase': 235,\n",
              " 'over': 236,\n",
              " 'installments': 237,\n",
              " 'aug': 238,\n",
              " '2024': 239,\n",
              " 'why': 240,\n",
              " 'lifetime': 241,\n",
              " 'because': 242,\n",
              " 'low': 243,\n",
              " 'reach': 244,\n",
              " 'out': 245,\n",
              " 'fill': 246,\n",
              " 'team': 247,\n",
              " 'still': 248,\n",
              " 'ask': 249,\n",
              " 'doubts': 250,\n",
              " 'select': 251,\n",
              " 'gmai': 252,\n",
              " 'criteria': 253,\n",
              " 'there': 254,\n",
              " 'criterias': 255,\n",
              " 'attempt': 256,\n",
              " 'assessments': 257,\n",
              " 'joining': 258,\n",
              " 'current': 259,\n",
              " 'clarify': 260,\n",
              " 'does': 261,\n",
              " 'mean': 262,\n",
              " 'dont': 263,\n",
              " 'any': 264,\n",
              " 'jobs': 265,\n",
              " 'matter': 266,\n",
              " 'interview': 267,\n",
              " 'calls': 268,\n",
              " 'planning': 269,\n",
              " 'placements': 270,\n",
              " 'afraid': 271,\n",
              " 'disappointed': 272,\n",
              " 'portfolio': 273,\n",
              " 'building': 274,\n",
              " 'soft': 275,\n",
              " 'skill': 276,\n",
              " 'industry': 277,\n",
              " 'mentors': 278,\n",
              " 'discussion': 279,\n",
              " 'job': 280,\n",
              " 'hunting': 281,\n",
              " 'strategies': 282}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}